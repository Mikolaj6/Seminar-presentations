{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seminar.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52rgd3WB_xqs"
      },
      "source": [
        "First load scraping results and model from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVVavQHWNAty"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path_to_scraping_results='drive/My Drive/Seminar/results'\n",
        "roberta_base_transformers='drive/My Drive/Seminar/roberta_base_transformers'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27ogSrQuigi0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b553b1-4362-4211-c92b-25eff16e9bd7"
      },
      "source": [
        "!cp 'drive/My Drive/Seminar/results' .\n",
        "!cp 'drive/My Drive/Seminar/roberta_base_transformers.zip' .\n",
        "!unzip 'roberta_base_transformers.zip' -d  'roberta'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  roberta_base_transformers.zip\n",
            "  inflating: roberta/config.json     \n",
            "  inflating: roberta/pytorch_model.bin  \n",
            "  inflating: roberta/tokenizer.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBwfXEHli0Qa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d77a4cfb-7ee6-4d4d-e6dc-c09c8bf971bb"
      },
      "source": [
        "!ls roberta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config.json  pytorch_model.bin\ttokenizer.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38MXAuULlW9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ed40f3-5d28-413f-a647-844535b111aa"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install xgboost\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "import os\n",
        "from transformers import RobertaModel, AutoModel, PreTrainedTokenizerFast\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import xgboost as xgb\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmD_oGA8_ubn"
      },
      "source": [
        "with open ('results', 'rb') as fp:\n",
        "    itemlist = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM-4NTK4BASF"
      },
      "source": [
        "X_list = itemlist['x_original']\n",
        "Y_list = itemlist['y']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtu2rio-kkjT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba0edf6-830f-4980-c0a5-13bc146c013a"
      },
      "source": [
        "print('X_list:', len(X_list), 'Y_list:', len(Y_list), '\\n')\n",
        "\n",
        "print('Przykładowe zawartości stron\\n')\n",
        "print(X_list[29][:120])\n",
        "print(X_list[27][:120])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_list: 85 Y_list: 85 \n",
            "\n",
            "Przykładowe zawartości stron\n",
            "\n",
            "Warta wprowadziła nową ofertą ubezpieczeń komunikacyjnych - Bankier.pl RORWarta wprowadziła nową ofertą ubezpieczeń komu\n",
            "Warta stworzyła ofertę gwarancji środowiskowej dla małych i średnich firm - Money.plTrwa ładowanie...Biznes mówiGiełdaNo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOZ1hlcNJ5Yr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a82fb556-df39-40aa-f72b-8d2f7a1f16f4"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X_list)\n",
        "print(len(vectorizer.get_feature_names()))\n",
        "\n",
        "train_X_tfidf, test_X_tfidf, train_Y_tfidf, test_Y_tfidf = \\\n",
        "  train_test_split(np.array(X.todense()), np.array(Y_list).astype(int),\n",
        "                   random_state=2137,\n",
        "                   test_size=0.3,\n",
        "                   stratify=Y_list)\n",
        "\n",
        "D_train = xgb.DMatrix(train_X_tfidf, label=train_Y_tfidf)\n",
        "D_test = xgb.DMatrix(test_X_tfidf, label=test_Y_tfidf)\n",
        "\n",
        "param = {\n",
        "    'eta': 0.3, \n",
        "    'max_depth': 6,  \n",
        "    'objective': 'multi:softprob',  \n",
        "    'num_class': 4} \n",
        "\n",
        "steps_xgb = 20\n",
        "\n",
        "model_xgb = xgb.train(param, D_train, steps_xgb)\n",
        "\n",
        "with open('xgb_model.pkl', 'wb') as model_xgb_file:\n",
        "  pickle.dump(model_xgb, model_xgb_file)\n",
        "\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as tfidf_vectorizer_file:\n",
        "  pickle.dump(vectorizer, tfidf_vectorizer_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26291\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5DLEetqHeNm"
      },
      "source": [
        "train_text, tmp_text, train_labels, tmp_labels = train_test_split(np.array(X_list), np.array(Y_list).astype(int),\n",
        "                                                                 random_state=2020,\n",
        "                                                                 test_size=0.3,\n",
        "                                                                 stratify=Y_list)\n",
        "\n",
        "test_text, val_text, test_labels, val_labels = train_test_split(tmp_text, tmp_labels,\n",
        "                                                                 random_state=2020,\n",
        "                                                                 test_size=0.5,\n",
        "                                                                 stratify=tmp_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtXSSYg_G486"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2E6wbsnHuqh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "eee44d18-69f0-47b2-af7d-5520bb720901"
      },
      "source": [
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff1d773f550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu2ItnEdIYoO"
      },
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "model_dir = \"roberta\"\n",
        "tokenizer = PreTrainedTokenizerFast(tokenizer_file=os.path.join(model_dir, \"tokenizer.json\"))\n",
        "tokenizer.pad_token = '<pad>'\n",
        "roberta: RobertaModel = AutoModel.from_pretrained(model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVwOuMc7iCd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f614b685-53ba-41ba-f11d-b4fafd13c3a9"
      },
      "source": [
        "roberta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50001, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): RobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): RobertaPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDp-mK2poKfN"
      },
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = 500,\n",
        "    truncation=True,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = 500,\n",
        "    truncation=True,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = 500,\n",
        "    truncation=True,\n",
        "    padding=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXvVgGGGqfjq"
      },
      "source": [
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlBPaaHehXX0"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 10\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4jj9LHmi1qx"
      },
      "source": [
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frf2SjBjjFGw"
      },
      "source": [
        "\n",
        "# freeze all the parameters\n",
        "for param in roberta.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "surP1KJ0jN3y"
      },
      "source": [
        "class ROBERTa_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, roberta_):\n",
        "      \n",
        "      super(ROBERTa_Arch, self).__init__()\n",
        "\n",
        "      self.roberta = roberta_\n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.roberta(sent_id, attention_mask=mask)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPApGrGSmRoE"
      },
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = ROBERTa_Arch(roberta)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb6qdcaRoMSE"
      },
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 0.0002)          # learning rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnJ8VrTknZut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc6a291-ecfe-4f93-f431-95650d591ae2"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "\n",
        "print(\"Class Weights:\",class_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class Weights: [0.84285714 1.22916667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAseDerMog6g"
      },
      "source": [
        "# converting list of class weights to a tensor\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# push to GPU\n",
        "weights = weights.to(device)\n",
        "\n",
        "# define the loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 120"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR8tPILGpGvy"
      },
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ftywJxmq0hr"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LeVkrkrMFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49282feb-5313-41f6-954a-9de1726fd2e1"
      },
      "source": [
        "\n",
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.711\n",
            "Validation Loss: 0.749\n",
            "\n",
            " Epoch 2 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.692\n",
            "Validation Loss: 0.677\n",
            "\n",
            " Epoch 3 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.679\n",
            "Validation Loss: 0.660\n",
            "\n",
            " Epoch 4 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.675\n",
            "Validation Loss: 0.646\n",
            "\n",
            " Epoch 5 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.669\n",
            "Validation Loss: 0.649\n",
            "\n",
            " Epoch 6 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.650\n",
            "Validation Loss: 0.668\n",
            "\n",
            " Epoch 7 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.649\n",
            "Validation Loss: 0.688\n",
            "\n",
            " Epoch 8 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.635\n",
            "Validation Loss: 0.648\n",
            "\n",
            " Epoch 9 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.637\n",
            "Validation Loss: 0.606\n",
            "\n",
            " Epoch 10 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.641\n",
            "Validation Loss: 0.622\n",
            "\n",
            " Epoch 11 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.626\n",
            "Validation Loss: 0.652\n",
            "\n",
            " Epoch 12 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.615\n",
            "Validation Loss: 0.597\n",
            "\n",
            " Epoch 13 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.621\n",
            "Validation Loss: 0.588\n",
            "\n",
            " Epoch 14 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.593\n",
            "Validation Loss: 0.605\n",
            "\n",
            " Epoch 15 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.588\n",
            "Validation Loss: 0.643\n",
            "\n",
            " Epoch 16 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.590\n",
            "Validation Loss: 0.597\n",
            "\n",
            " Epoch 17 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.578\n",
            "Validation Loss: 0.581\n",
            "\n",
            " Epoch 18 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.572\n",
            "Validation Loss: 0.569\n",
            "\n",
            " Epoch 19 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.564\n",
            "Validation Loss: 0.556\n",
            "\n",
            " Epoch 20 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.561\n",
            "Validation Loss: 0.525\n",
            "\n",
            " Epoch 21 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.553\n",
            "Validation Loss: 0.534\n",
            "\n",
            " Epoch 22 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.526\n",
            "Validation Loss: 0.570\n",
            "\n",
            " Epoch 23 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.531\n",
            "Validation Loss: 0.559\n",
            "\n",
            " Epoch 24 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.505\n",
            "Validation Loss: 0.552\n",
            "\n",
            " Epoch 25 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.503\n",
            "Validation Loss: 0.512\n",
            "\n",
            " Epoch 26 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.478\n",
            "Validation Loss: 0.474\n",
            "\n",
            " Epoch 27 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.478\n",
            "Validation Loss: 0.496\n",
            "\n",
            " Epoch 28 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.459\n",
            "Validation Loss: 0.544\n",
            "\n",
            " Epoch 29 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.472\n",
            "Validation Loss: 0.464\n",
            "\n",
            " Epoch 30 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.466\n",
            "Validation Loss: 0.481\n",
            "\n",
            " Epoch 31 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.491\n",
            "Validation Loss: 0.474\n",
            "\n",
            " Epoch 32 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.478\n",
            "Validation Loss: 0.571\n",
            "\n",
            " Epoch 33 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.459\n",
            "Validation Loss: 0.496\n",
            "\n",
            " Epoch 34 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.461\n",
            "Validation Loss: 0.480\n",
            "\n",
            " Epoch 35 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.441\n",
            "Validation Loss: 0.459\n",
            "\n",
            " Epoch 36 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.436\n",
            "Validation Loss: 0.541\n",
            "\n",
            " Epoch 37 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.494\n",
            "Validation Loss: 0.422\n",
            "\n",
            " Epoch 38 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.417\n",
            "Validation Loss: 0.438\n",
            "\n",
            " Epoch 39 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.458\n",
            "Validation Loss: 0.455\n",
            "\n",
            " Epoch 40 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.460\n",
            "Validation Loss: 0.452\n",
            "\n",
            " Epoch 41 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.404\n",
            "Validation Loss: 0.475\n",
            "\n",
            " Epoch 42 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.446\n",
            "Validation Loss: 0.501\n",
            "\n",
            " Epoch 43 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.402\n",
            "Validation Loss: 0.440\n",
            "\n",
            " Epoch 44 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.414\n",
            "Validation Loss: 0.475\n",
            "\n",
            " Epoch 45 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.376\n",
            "Validation Loss: 0.568\n",
            "\n",
            " Epoch 46 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.423\n",
            "Validation Loss: 0.489\n",
            "\n",
            " Epoch 47 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.385\n",
            "Validation Loss: 0.494\n",
            "\n",
            " Epoch 48 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.369\n",
            "Validation Loss: 0.414\n",
            "\n",
            " Epoch 49 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.363\n",
            "Validation Loss: 0.420\n",
            "\n",
            " Epoch 50 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.389\n",
            "Validation Loss: 0.458\n",
            "\n",
            " Epoch 51 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.368\n",
            "Validation Loss: 0.476\n",
            "\n",
            " Epoch 52 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.378\n",
            "Validation Loss: 0.403\n",
            "\n",
            " Epoch 53 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.444\n",
            "Validation Loss: 0.388\n",
            "\n",
            " Epoch 54 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.351\n",
            "Validation Loss: 0.504\n",
            "\n",
            " Epoch 55 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.341\n",
            "Validation Loss: 0.545\n",
            "\n",
            " Epoch 56 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.356\n",
            "Validation Loss: 0.457\n",
            "\n",
            " Epoch 57 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.346\n",
            "Validation Loss: 0.406\n",
            "\n",
            " Epoch 58 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.366\n",
            "Validation Loss: 0.581\n",
            "\n",
            " Epoch 59 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.340\n",
            "Validation Loss: 0.476\n",
            "\n",
            " Epoch 60 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.345\n",
            "Validation Loss: 0.368\n",
            "\n",
            " Epoch 61 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.344\n",
            "Validation Loss: 0.432\n",
            "\n",
            " Epoch 62 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.323\n",
            "Validation Loss: 0.549\n",
            "\n",
            " Epoch 63 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.317\n",
            "Validation Loss: 0.447\n",
            "\n",
            " Epoch 64 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.382\n",
            "Validation Loss: 0.415\n",
            "\n",
            " Epoch 65 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.318\n",
            "Validation Loss: 0.476\n",
            "\n",
            " Epoch 66 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.303\n",
            "Validation Loss: 0.539\n",
            "\n",
            " Epoch 67 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.325\n",
            "Validation Loss: 0.444\n",
            "\n",
            " Epoch 68 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.311\n",
            "Validation Loss: 0.407\n",
            "\n",
            " Epoch 69 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.362\n",
            "Validation Loss: 0.400\n",
            "\n",
            " Epoch 70 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.276\n",
            "Validation Loss: 0.509\n",
            "\n",
            " Epoch 71 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.321\n",
            "Validation Loss: 0.428\n",
            "\n",
            " Epoch 72 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.314\n",
            "Validation Loss: 0.488\n",
            "\n",
            " Epoch 73 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.338\n",
            "Validation Loss: 0.498\n",
            "\n",
            " Epoch 74 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.280\n",
            "Validation Loss: 0.402\n",
            "\n",
            " Epoch 75 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.322\n",
            "Validation Loss: 0.367\n",
            "\n",
            " Epoch 76 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.296\n",
            "Validation Loss: 0.390\n",
            "\n",
            " Epoch 77 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.300\n",
            "Validation Loss: 0.422\n",
            "\n",
            " Epoch 78 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.256\n",
            "Validation Loss: 0.439\n",
            "\n",
            " Epoch 79 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.313\n",
            "Validation Loss: 0.376\n",
            "\n",
            " Epoch 80 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.242\n",
            "Validation Loss: 0.423\n",
            "\n",
            " Epoch 81 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.298\n",
            "Validation Loss: 0.389\n",
            "\n",
            " Epoch 82 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.277\n",
            "Validation Loss: 0.413\n",
            "\n",
            " Epoch 83 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.302\n",
            "Validation Loss: 0.566\n",
            "\n",
            " Epoch 84 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.301\n",
            "Validation Loss: 0.506\n",
            "\n",
            " Epoch 85 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.256\n",
            "Validation Loss: 0.427\n",
            "\n",
            " Epoch 86 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.302\n",
            "Validation Loss: 0.378\n",
            "\n",
            " Epoch 87 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.279\n",
            "Validation Loss: 0.478\n",
            "\n",
            " Epoch 88 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.271\n",
            "Validation Loss: 0.503\n",
            "\n",
            " Epoch 89 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.259\n",
            "Validation Loss: 0.476\n",
            "\n",
            " Epoch 90 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.249\n",
            "Validation Loss: 0.387\n",
            "\n",
            " Epoch 91 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.289\n",
            "Validation Loss: 0.485\n",
            "\n",
            " Epoch 92 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.215\n",
            "Validation Loss: 0.492\n",
            "\n",
            " Epoch 93 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.259\n",
            "Validation Loss: 0.403\n",
            "\n",
            " Epoch 94 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.232\n",
            "Validation Loss: 0.433\n",
            "\n",
            " Epoch 95 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.223\n",
            "Validation Loss: 0.428\n",
            "\n",
            " Epoch 96 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.286\n",
            "Validation Loss: 0.355\n",
            "\n",
            " Epoch 97 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.257\n",
            "Validation Loss: 0.412\n",
            "\n",
            " Epoch 98 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.236\n",
            "Validation Loss: 0.428\n",
            "\n",
            " Epoch 99 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.319\n",
            "Validation Loss: 0.405\n",
            "\n",
            " Epoch 100 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.227\n",
            "Validation Loss: 0.383\n",
            "\n",
            " Epoch 101 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.258\n",
            "Validation Loss: 0.449\n",
            "\n",
            " Epoch 102 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.290\n",
            "Validation Loss: 0.436\n",
            "\n",
            " Epoch 103 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.209\n",
            "Validation Loss: 0.383\n",
            "\n",
            " Epoch 104 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.231\n",
            "Validation Loss: 0.419\n",
            "\n",
            " Epoch 105 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.214\n",
            "Validation Loss: 0.409\n",
            "\n",
            " Epoch 106 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.186\n",
            "Validation Loss: 0.400\n",
            "\n",
            " Epoch 107 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.240\n",
            "Validation Loss: 0.461\n",
            "\n",
            " Epoch 108 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.241\n",
            "Validation Loss: 0.449\n",
            "\n",
            " Epoch 109 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.199\n",
            "Validation Loss: 0.456\n",
            "\n",
            " Epoch 110 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.301\n",
            "Validation Loss: 0.401\n",
            "\n",
            " Epoch 111 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.261\n",
            "Validation Loss: 0.400\n",
            "\n",
            " Epoch 112 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.237\n",
            "Validation Loss: 0.488\n",
            "\n",
            " Epoch 113 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.262\n",
            "Validation Loss: 0.396\n",
            "\n",
            " Epoch 114 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.235\n",
            "Validation Loss: 0.428\n",
            "\n",
            " Epoch 115 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.284\n",
            "Validation Loss: 0.520\n",
            "\n",
            " Epoch 116 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.249\n",
            "Validation Loss: 0.393\n",
            "\n",
            " Epoch 117 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.219\n",
            "Validation Loss: 0.477\n",
            "\n",
            " Epoch 118 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.274\n",
            "Validation Loss: 0.480\n",
            "\n",
            " Epoch 119 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.204\n",
            "Validation Loss: 0.412\n",
            "\n",
            " Epoch 120 / 120\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.231\n",
            "Validation Loss: 0.485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WnpnWk7rQ6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7cf1ecd-9319-49ad-c0b7-4af27566f011"
      },
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkG2MPOX3ON3"
      },
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb2UkYtIvROC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063e6e55-0682-4ece-ebc7-180c55852d9c"
      },
      "source": [
        "preds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff9SJQOkvUxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfa8e1f5-5ffe-4933-cc6f-7331029ea4be"
      },
      "source": [
        "test_y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsy_iKO03Q3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f15c0df6-bc2c-4605-870f-77ef90891060"
      },
      "source": [
        "print('ROBERTA')\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROBERTA\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.71      0.83         7\n",
            "           1       0.75      1.00      0.86         6\n",
            "\n",
            "    accuracy                           0.85        13\n",
            "   macro avg       0.88      0.86      0.85        13\n",
            "weighted avg       0.88      0.85      0.84        13\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIymJ2pE8DOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c186a640-a890-4221-ecb2-8d7db208a359"
      },
      "source": [
        "print('Most common value only')\n",
        "print(classification_report(test_y, np.zeros(preds.shape[0], dtype=int)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most common value only\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      1.00      0.70         7\n",
            "           1       0.00      0.00      0.00         6\n",
            "\n",
            "    accuracy                           0.54        13\n",
            "   macro avg       0.27      0.50      0.35        13\n",
            "weighted avg       0.29      0.54      0.38        13\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8pO4KpEn6DM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f4e124-c75b-4f22-e256-0f5e2094d3ad"
      },
      "source": [
        "\n",
        "preds_xgb = model_xgb.predict(D_test)\n",
        "best_preds_xgb = np.asarray([np.argmax(line) for line in preds_xgb])\n",
        "print('XGB model')\n",
        "print(classification_report(best_preds_xgb, test_Y_tfidf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XGB model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.88      0.94        17\n",
            "           1       0.82      1.00      0.90         9\n",
            "\n",
            "    accuracy                           0.92        26\n",
            "   macro avg       0.91      0.94      0.92        26\n",
            "weighted avg       0.94      0.92      0.92        26\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Pw3pKrH3V_a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "10cd2725-2311-4c02-9fac-2a6b66bd8b5b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "y_actu = pd.Series(test_y, name='Actual')\n",
        "y_pred = pd.Series(preds, name='Predicted')\n",
        "\n",
        "df_confusion = pd.crosstab(y_actu, y_pred)\n",
        "df_confusion"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Predicted</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Actual</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Predicted  0  1\n",
              "Actual         \n",
              "0          5  2\n",
              "1          0  6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-OTW28S3cZC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "25b4f262-8309-474d-da33-400d224fe0ee"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_confusion_matrix(df_confusion, title='Confusion matrix', cmap=plt.cm.coolwarm):\n",
        "    plt.matshow(df_confusion, cmap=cmap) # imshow\n",
        "    #plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(df_confusion.columns))\n",
        "    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
        "    plt.yticks(tick_marks, df_confusion.index)\n",
        "    #plt.tight_layout()\n",
        "    plt.ylabel(df_confusion.index.name)\n",
        "    plt.xlabel(df_confusion.columns.name)\n",
        "\n",
        "plot_confusion_matrix(df_confusion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7vz08qw3fsa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "9c5cb4e2-9f87-4319-961d-dd30a9c96c57"
      },
      "source": [
        "plt.plot(train_losses)\n",
        "plt.plot(valid_losses)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLWMwRUQ6fta"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}